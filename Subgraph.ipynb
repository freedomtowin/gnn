{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "st_d10_th0.001_lr0.001\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'state' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-dbd6eef29893>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m30\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[0mout_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGNNLayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marcnode_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m             \u001b[0mloss_value_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mout_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\gnn\\gnn\\GNN.py\u001b[0m in \u001b[0;36mpredict_node\u001b[1;34m(self, comp_inp, ArcNode, NodeGraph)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0mstate_old_init\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mArcNode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m         \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_init\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m         \u001b[0mstate_old\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_old_init\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'state' referenced before assignment"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gnn.gnn_utils as gnn_utils\n",
    "from gnn.GNN import GNN as GraphNetwork\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "##### GPU & stuff config\n",
    "import os\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "data_path = \"./data\"\n",
    "\n",
    "set_name = \"sub_15_7_200\"\n",
    "############# training set ################\n",
    "\n",
    "\n",
    "#inp, arcnode, nodegraph, nodein, labels = Library.set_load_subgraph(data_path, \"train\")\n",
    "inp, arcnode, nodegraph, nodein, labels, _ = gnn_utils.set_load_general(data_path, \"train\", set_name=set_name)\n",
    "############ test set ####################\n",
    "\n",
    "#inp_test, arcnode_test, nodegraph_test, nodein_test, labels_test = Library.set_load_subgraph(data_path, \"test\")\n",
    "inp_test, arcnode_test, nodegraph_test, nodein_test, labels_test, _ = gnn_utils.set_load_general(data_path, \"test\", set_name=set_name)\n",
    "\n",
    "############ validation set #############\n",
    "\n",
    "#inp_val, arcnode_val, nodegraph_val, nodein_val, labels_val = Library.set_load_subgraph(data_path, \"valid\")\n",
    "inp_val, arcnode_val, nodegraph_val, nodein_val, labels_val, _ = gnn_utils.set_load_general(data_path, \"validation\", set_name=set_name)\n",
    "\n",
    "EPSILON = 0.00000001\n",
    "\n",
    "@tf.function()\n",
    "def loss(target,output):\n",
    "    target = tf.cast(target,tf.float32)\n",
    "    output = tf.maximum(output, EPSILON, name=\"Avoiding_explosions\")  # to avoid explosions\n",
    "    xent = -tf.reduce_sum(target * tf.math.log(output), 1)\n",
    "    lo = tf.reduce_mean(xent)\n",
    "    return lo\n",
    "\n",
    "@tf.function()\n",
    "def metric(output, target):\n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(target, 1))\n",
    "    metric = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "    return metric\n",
    "\n",
    "\n",
    "inp = inp[0]\n",
    "\n",
    "arcnode=arcnode[0]\n",
    "\n",
    "nodegraph=nodegraph[0]\n",
    "\n",
    "inp_val = inp_val[0]\n",
    "\n",
    "arcnode_val = arcnode_val[0]\n",
    "\n",
    "nodegraph_val=nodegraph_val[0]\n",
    "\n",
    "# set input and output dim, the maximum number of iterations, the number of epochs and the optimizer\n",
    "\n",
    "threshold = 0.001\n",
    "learning_rate = 0.001\n",
    "state_dim = 10\n",
    "\n",
    "input_dim = len(inp[0])\n",
    "output_dim = 2\n",
    "max_it = 50\n",
    "num_epoch = 100\n",
    "\n",
    "def create_model():\n",
    "\n",
    "    comp_inp = tf.keras.Input(shape=(input_dim), name=\"input\")\n",
    "    \n",
    "    layer = GraphNetwork(input_dim, state_dim, output_dim,                             \n",
    "                         hidden_state_dim = 5, hidden_output_dim = 5,\n",
    "                         ArcNode=arcnode,NodeGraph=None,threshold=threshold)\n",
    "    \n",
    "    output = layer(comp_inp)\n",
    "    \n",
    "    model = tf.keras.Model(comp_inp, output)\n",
    "\n",
    "    return model,layer\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model,GNNLayer = create_model()\n",
    "\n",
    "# initialize GNN\n",
    "param = \"st_d\" + str(state_dim) + \"_th\" + str(threshold) + \"_lr\" + str(learning_rate)\n",
    "print(param)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "for count in range(0, num_epoch):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        out = model(inp.astype(np.float32),training=True)\n",
    "\n",
    "        loss_value = loss(labels,out)\n",
    "\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if count % 30 == 0:\n",
    "            out_val = GNNLayer.predict_node(inp_val.astype(np.float32), arcnode_val)\n",
    "            loss_value_val = loss(labels_val,out_val)\n",
    "            \n",
    "            print(\"Epoch \", count)\n",
    "            print(\"Training: \", loss_value.numpy())\n",
    "            print(\"Validation: \",loss_value_val.numpy())\n",
    "\n",
    "        count = count + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcnode_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "12/np.sum(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "datasci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
