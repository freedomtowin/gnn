{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading karate club dataset...\n",
      "st_d2_th0.001_lr0.0001\n",
      "Epoch  0\n",
      "Training:  1.4895287\n",
      "Test:  1.4496232\n",
      "Epoch  10\n",
      "Training:  1.4709486\n",
      "Test:  1.4484726\n",
      "Epoch  20\n",
      "Training:  1.451918\n",
      "Test:  1.4479373\n",
      "Epoch  30\n",
      "Training:  1.4323565\n",
      "Test:  1.4481275\n",
      "Epoch  40\n",
      "Training:  1.4122018\n",
      "Test:  1.4492273\n",
      "Epoch  50\n",
      "Training:  1.3914547\n",
      "Test:  1.4514649\n",
      "Epoch  60\n",
      "Training:  1.3702981\n",
      "Test:  1.4550579\n",
      "Epoch  70\n",
      "Training:  1.3493625\n",
      "Test:  1.4600443\n",
      "Epoch  80\n",
      "Training:  1.3300219\n",
      "Test:  1.4660459\n",
      "Epoch  90\n",
      "Training:  1.3140364\n",
      "Test:  1.4719964\n",
      "Epoch  100\n",
      "Training:  1.3020201\n",
      "Test:  1.4761168\n",
      "Epoch  110\n",
      "Training:  1.2924347\n",
      "Test:  1.4768645\n",
      "Epoch  120\n",
      "Training:  1.2834069\n",
      "Test:  1.4743016\n",
      "Epoch  130\n",
      "Training:  1.2744728\n",
      "Test:  1.4699994\n",
      "Epoch  140\n",
      "Training:  1.2657646\n",
      "Test:  1.4655848\n",
      "Epoch  150\n",
      "Training:  1.2572708\n",
      "Test:  1.461812\n",
      "Epoch  160\n",
      "Training:  1.2489384\n",
      "Test:  1.4586589\n",
      "Epoch  170\n",
      "Training:  1.2407547\n",
      "Test:  1.455803\n",
      "Epoch  180\n",
      "Training:  1.2327147\n",
      "Test:  1.4529902\n",
      "Epoch  190\n",
      "Training:  1.2248096\n",
      "Test:  1.4501517\n",
      "Epoch  200\n",
      "Training:  1.2170331\n",
      "Test:  1.4473389\n",
      "Epoch  210\n",
      "Training:  1.2093801\n",
      "Test:  1.4446117\n",
      "Epoch  220\n",
      "Training:  1.2018465\n",
      "Test:  1.441987\n",
      "Epoch  230\n",
      "Training:  1.1944282\n",
      "Test:  1.439448\n",
      "Epoch  240\n",
      "Training:  1.1871222\n",
      "Test:  1.4369756\n",
      "Epoch  250\n",
      "Training:  1.1799254\n",
      "Test:  1.4345627\n",
      "Epoch  260\n",
      "Training:  1.1728352\n",
      "Test:  1.4322106\n",
      "Epoch  270\n",
      "Training:  1.1658489\n",
      "Test:  1.4299196\n",
      "Epoch  280\n",
      "Training:  1.158964\n",
      "Test:  1.4276865\n",
      "Epoch  290\n",
      "Training:  1.1521784\n",
      "Test:  1.4255068\n",
      "Epoch  300\n",
      "Training:  1.145489\n",
      "Test:  1.4233775\n",
      "Epoch  310\n",
      "Training:  1.1388938\n",
      "Test:  1.4212962\n",
      "Epoch  320\n",
      "Training:  1.1323899\n",
      "Test:  1.4192603\n",
      "Epoch  330\n",
      "Training:  1.1259744\n",
      "Test:  1.4172673\n",
      "Epoch  340\n",
      "Training:  1.119644\n",
      "Test:  1.4153147\n",
      "Epoch  350\n",
      "Training:  1.113395\n",
      "Test:  1.4133997\n",
      "Epoch  360\n",
      "Training:  1.1072233\n",
      "Test:  1.4115199\n",
      "Epoch  370\n",
      "Training:  1.1011229\n",
      "Test:  1.4096732\n",
      "Epoch  380\n",
      "Training:  1.0950882\n",
      "Test:  1.4078573\n",
      "Epoch  390\n",
      "Training:  1.0891114\n",
      "Test:  1.4060698\n",
      "Epoch  400\n",
      "Training:  1.0831832\n",
      "Test:  1.4043086\n",
      "Epoch  410\n",
      "Training:  1.0772923\n",
      "Test:  1.4025717\n",
      "Epoch  420\n",
      "Training:  1.0714259\n",
      "Test:  1.400857\n",
      "Epoch  430\n",
      "Training:  1.0655683\n",
      "Test:  1.3991632\n",
      "Epoch  440\n",
      "Training:  1.0597025\n",
      "Test:  1.3974887\n",
      "Epoch  450\n",
      "Training:  1.0538098\n",
      "Test:  1.3958325\n",
      "Epoch  460\n",
      "Training:  1.0478714\n",
      "Test:  1.394194\n",
      "Epoch  470\n",
      "Training:  1.0418695\n",
      "Test:  1.3925729\n",
      "Epoch  480\n",
      "Training:  1.0357901\n",
      "Test:  1.3909698\n",
      "Epoch  490\n",
      "Training:  1.0296233\n",
      "Test:  1.3893859\n",
      "Epoch  500\n",
      "Training:  1.0233656\n",
      "Test:  1.3878227\n",
      "Epoch  510\n",
      "Training:  1.0170205\n",
      "Test:  1.3862822\n",
      "Epoch  520\n",
      "Training:  1.0105972\n",
      "Test:  1.3847659\n",
      "Epoch  530\n",
      "Training:  1.00411\n",
      "Test:  1.3832757\n",
      "Epoch  540\n",
      "Training:  0.99757653\n",
      "Test:  1.3818134\n",
      "Epoch  550\n",
      "Training:  0.9910159\n",
      "Test:  1.3803794\n",
      "Epoch  560\n",
      "Training:  0.98444694\n",
      "Test:  1.3789749\n",
      "Epoch  570\n",
      "Training:  0.9778871\n",
      "Test:  1.3775994\n",
      "Epoch  580\n",
      "Training:  0.971352\n",
      "Test:  1.376253\n",
      "Epoch  590\n",
      "Training:  0.96485454\n",
      "Test:  1.3749356\n",
      "Epoch  600\n",
      "Training:  0.95840544\n",
      "Test:  1.3736459\n",
      "Epoch  610\n",
      "Training:  0.9520135\n",
      "Test:  1.3723832\n",
      "Epoch  620\n",
      "Training:  0.9456848\n",
      "Test:  1.3711469\n",
      "Epoch  630\n",
      "Training:  0.93942434\n",
      "Test:  1.3699358\n",
      "Epoch  640\n",
      "Training:  0.9332353\n",
      "Test:  1.3687487\n",
      "Epoch  650\n",
      "Training:  0.92711985\n",
      "Test:  1.3675845\n",
      "Epoch  660\n",
      "Training:  0.92107946\n",
      "Test:  1.3664426\n",
      "Epoch  670\n",
      "Training:  0.9151146\n",
      "Test:  1.3653215\n",
      "Epoch  680\n",
      "Training:  0.909225\n",
      "Test:  1.3642206\n",
      "Epoch  690\n",
      "Training:  0.9034105\n",
      "Test:  1.3631389\n",
      "Epoch  700\n",
      "Training:  0.89767015\n",
      "Test:  1.3620756\n",
      "Epoch  710\n",
      "Training:  0.8920031\n",
      "Test:  1.3610296\n",
      "Epoch  720\n",
      "Training:  0.886408\n",
      "Test:  1.3600003\n",
      "Epoch  730\n",
      "Training:  0.88088363\n",
      "Test:  1.3589872\n",
      "Epoch  740\n",
      "Training:  0.8754285\n",
      "Test:  1.3579893\n",
      "Epoch  750\n",
      "Training:  0.8700413\n",
      "Test:  1.3570063\n",
      "Epoch  760\n",
      "Training:  0.86472076\n",
      "Test:  1.3560373\n",
      "Epoch  770\n",
      "Training:  0.859465\n",
      "Test:  1.3550819\n",
      "Epoch  780\n",
      "Training:  0.854273\n",
      "Test:  1.3541394\n",
      "Epoch  790\n",
      "Training:  0.84914327\n",
      "Test:  1.3532096\n",
      "Epoch  800\n",
      "Training:  0.84407425\n",
      "Test:  1.3522918\n",
      "Epoch  810\n",
      "Training:  0.83906466\n",
      "Test:  1.3513855\n",
      "Epoch  820\n",
      "Training:  0.8341133\n",
      "Test:  1.3504903\n",
      "Epoch  830\n",
      "Training:  0.8292188\n",
      "Test:  1.349606\n",
      "Epoch  840\n",
      "Training:  0.82438\n",
      "Test:  1.3487319\n",
      "Epoch  850\n",
      "Training:  0.81959546\n",
      "Test:  1.3478676\n",
      "Epoch  860\n",
      "Training:  0.8148643\n",
      "Test:  1.3470129\n",
      "Epoch  870\n",
      "Training:  0.81018513\n",
      "Test:  1.3461673\n",
      "Epoch  880\n",
      "Training:  0.8055569\n",
      "Test:  1.3453302\n",
      "Epoch  890\n",
      "Training:  0.8009787\n",
      "Test:  1.3445019\n",
      "Epoch  900\n",
      "Training:  0.7964493\n",
      "Test:  1.3436811\n",
      "Epoch  910\n",
      "Training:  0.7919677\n",
      "Test:  1.3428681\n",
      "Epoch  920\n",
      "Training:  0.78753304\n",
      "Test:  1.3420625\n",
      "Epoch  930\n",
      "Training:  0.78314424\n",
      "Test:  1.341264\n",
      "Epoch  940\n",
      "Training:  0.7788004\n",
      "Test:  1.3404721\n",
      "Epoch  950\n",
      "Training:  0.7745007\n",
      "Test:  1.3396865\n",
      "Epoch  960\n",
      "Training:  0.7702442\n",
      "Test:  1.338907\n",
      "Epoch  970\n",
      "Training:  0.76603\n",
      "Test:  1.3381336\n",
      "Epoch  980\n",
      "Training:  0.7618574\n",
      "Test:  1.3373654\n",
      "Epoch  990\n",
      "Training:  0.7577254\n",
      "Test:  1.3366027\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gnn.gnn_utils as utils\n",
    "from gnn.GNN import GNN as GraphNetwork\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# import utils\n",
    "# import GNNs as GNN\n",
    "# import Net_Karate as n\n",
    "# from scipy.sparse import coo_matrix\n",
    "\n",
    "##### GPU & stuff config\n",
    "import os\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "\n",
    "############# training set ################\n",
    "\n",
    "\n",
    "E, N, labels,  mask_train, mask_test = utils.load_karate()\n",
    "inp, arcnode, graphnode = utils.from_EN_to_GNN(E, N)\n",
    "\n",
    "\n",
    "\n",
    "EPSILON = 0.00000001\n",
    "\n",
    "@tf.function()\n",
    "def loss(target,output,mask):\n",
    "    target = tf.cast(target,tf.float32)\n",
    "    output = tf.maximum(output, EPSILON, name=\"Avoiding_explosions\")  # to avoid explosions\n",
    "    xent = -tf.reduce_sum(target * tf.math.log(output), 1)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    xent *= mask\n",
    "    lo = tf.reduce_mean(xent)\n",
    "    return lo\n",
    "\n",
    "@tf.function()\n",
    "def metric(output, target,mask):\n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(target, 1))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    accuracy_all *= mask\n",
    "\n",
    "    return tf.reduce_mean(accuracy_all)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# set input and output dim, the maximum number of iterations, the number of epochs and the optimizer\n",
    "threshold = 0.001\n",
    "learning_rate = 0.0001\n",
    "state_dim = 2\n",
    "input_dim = inp.shape[1]\n",
    "output_dim = labels.shape[1]\n",
    "max_it = 50\n",
    "num_epoch = 1000\n",
    "\n",
    "def create_model():\n",
    "\n",
    "    comp_inp = tf.keras.Input(shape=(input_dim), name=\"input\")\n",
    "    \n",
    "    layer = GraphNetwork(input_dim, state_dim, output_dim,                             \n",
    "                         hidden_state_dim = 5, hidden_output_dim = 5,\n",
    "                         ArcNode=arcnode,NodeGraph=None,threshold=threshold)\n",
    "    \n",
    "    output = layer(comp_inp)\n",
    "    \n",
    "    model = tf.keras.Model(comp_inp, output)\n",
    "\n",
    "    return model,layer\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model,GNNLayer = create_model()\n",
    "\n",
    "# initialize GNN\n",
    "param = \"st_d\" + str(state_dim) + \"_th\" + str(threshold) + \"_lr\" + str(learning_rate)\n",
    "print(param)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "for count in range(num_epoch):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        out = model(inp,training=True)\n",
    "\n",
    "        loss_value = loss(labels,out, mask=mask_train)\n",
    "\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        if count % 10 == 0:\n",
    "            out_val = GNNLayer.predict_node(inp, arcnode)\n",
    "            loss_value_val = loss(labels,out, mask=mask_test)\n",
    "            \n",
    "            print(\"Epoch \", count)\n",
    "            print(\"Training: \", loss_value.numpy())\n",
    "            print(\"Test: \", loss_value_val.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(labels,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcnode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "datasci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
