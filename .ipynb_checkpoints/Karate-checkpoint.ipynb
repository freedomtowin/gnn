{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading karate club dataset...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gnn.gnn_utils as utils\n",
    "from gnn.GNN import GNN as GraphNetwork\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# import utils\n",
    "# import GNNs as GNN\n",
    "# import Net_Karate as n\n",
    "# from scipy.sparse import coo_matrix\n",
    "\n",
    "##### GPU & stuff config\n",
    "import os\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "\n",
    "############# training set ################\n",
    "\n",
    "\n",
    "E, N, labels,  mask_train, mask_test = utils.load_karate()\n",
    "inp, arcnode, graphnode = utils.from_EN_to_GNN(E, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 0.00000001\n",
    "\n",
    "@tf.function()\n",
    "def loss(target,output,mask):\n",
    "    target = tf.cast(target,tf.float32)\n",
    "    output = tf.maximum(output, EPSILON, name=\"Avoiding_explosions\")  # to avoid explosions\n",
    "    xent = -tf.reduce_sum(target * tf.math.log(output), 1)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    xent *= mask\n",
    "    lo = tf.reduce_mean(xent)\n",
    "    return lo\n",
    "\n",
    "@tf.function()\n",
    "def metric(output, target,mask):\n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(target, 1))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    accuracy_all *= mask\n",
    "\n",
    "    return tf.reduce_mean(accuracy_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "st_d2_th0.001_lr0.0001\n",
      "Epoch  0\n",
      "Training:  1.3891187\n",
      "Test:  0.46666667\n",
      "Epoch  10\n",
      "Training:  1.384643\n",
      "Test:  0.4\n",
      "Epoch  20\n",
      "Training:  1.3800702\n",
      "Test:  0.4\n",
      "Epoch  30\n",
      "Training:  1.375344\n",
      "Test:  0.36666667\n",
      "Epoch  40\n",
      "Training:  1.3704196\n",
      "Test:  0.36666667\n",
      "Epoch  50\n",
      "Training:  1.3652649\n",
      "Test:  0.36666667\n",
      "Epoch  60\n",
      "Training:  1.3598564\n",
      "Test:  0.36666667\n",
      "Epoch  70\n",
      "Training:  1.3541766\n",
      "Test:  0.36666667\n",
      "Epoch  80\n",
      "Training:  1.3482156\n",
      "Test:  0.36666667\n",
      "Epoch  90\n",
      "Training:  1.3419695\n",
      "Test:  0.36666667\n",
      "Epoch  100\n",
      "Training:  1.3354416\n",
      "Test:  0.36666667\n",
      "Epoch  110\n",
      "Training:  1.3286406\n",
      "Test:  0.33333334\n",
      "Epoch  120\n",
      "Training:  1.3215774\n",
      "Test:  0.33333334\n",
      "Epoch  130\n",
      "Training:  1.314266\n",
      "Test:  0.33333334\n",
      "Epoch  140\n",
      "Training:  1.3067214\n",
      "Test:  0.33333334\n",
      "Epoch  150\n",
      "Training:  1.2989595\n",
      "Test:  0.33333334\n",
      "Epoch  160\n",
      "Training:  1.2909964\n",
      "Test:  0.33333334\n",
      "Epoch  170\n",
      "Training:  1.2828491\n",
      "Test:  0.29999998\n",
      "Epoch  180\n",
      "Training:  1.2745337\n",
      "Test:  0.29999998\n",
      "Epoch  190\n",
      "Training:  1.2660676\n",
      "Test:  0.29999998\n",
      "Epoch  200\n",
      "Training:  1.2574668\n",
      "Test:  0.3333333\n",
      "Epoch  210\n",
      "Training:  1.2487481\n",
      "Test:  0.3333333\n",
      "Epoch  220\n",
      "Training:  1.2399275\n",
      "Test:  0.3333333\n",
      "Epoch  230\n",
      "Training:  1.2310208\n",
      "Test:  0.3333333\n",
      "Epoch  240\n",
      "Training:  1.2220436\n",
      "Test:  0.3333333\n",
      "Epoch  250\n",
      "Training:  1.213011\n",
      "Test:  0.3333333\n",
      "Epoch  260\n",
      "Training:  1.2039376\n",
      "Test:  0.3333333\n",
      "Epoch  270\n",
      "Training:  1.1948378\n",
      "Test:  0.3333333\n",
      "Epoch  280\n",
      "Training:  1.1857251\n",
      "Test:  0.3333333\n",
      "Epoch  290\n",
      "Training:  1.1766121\n",
      "Test:  0.3333333\n",
      "Epoch  300\n",
      "Training:  1.1675119\n",
      "Test:  0.3333333\n",
      "Epoch  310\n",
      "Training:  1.1584357\n",
      "Test:  0.3333333\n",
      "Epoch  320\n",
      "Training:  1.1493946\n",
      "Test:  0.29999998\n",
      "Epoch  330\n",
      "Training:  1.1403991\n",
      "Test:  0.29999998\n",
      "Epoch  340\n",
      "Training:  1.1314588\n",
      "Test:  0.29999998\n",
      "Epoch  350\n",
      "Training:  1.1225817\n",
      "Test:  0.29999998\n",
      "Epoch  360\n",
      "Training:  1.1137762\n",
      "Test:  0.26666665\n",
      "Epoch  370\n",
      "Training:  1.1050494\n",
      "Test:  0.26666665\n",
      "Epoch  380\n",
      "Training:  1.0964073\n",
      "Test:  0.26666665\n",
      "Epoch  390\n",
      "Training:  1.0878552\n",
      "Test:  0.26666665\n",
      "Epoch  400\n",
      "Training:  1.079398\n",
      "Test:  0.26666665\n",
      "Epoch  410\n",
      "Training:  1.0710393\n",
      "Test:  0.29999998\n",
      "Epoch  420\n",
      "Training:  1.0627822\n",
      "Test:  0.29999998\n",
      "Epoch  430\n",
      "Training:  1.0546288\n",
      "Test:  0.29999998\n",
      "Epoch  440\n",
      "Training:  1.0465813\n",
      "Test:  0.29999998\n",
      "Epoch  450\n",
      "Training:  1.03864\n",
      "Test:  0.26666665\n",
      "Epoch  460\n",
      "Training:  1.0308055\n",
      "Test:  0.26666665\n",
      "Epoch  470\n",
      "Training:  1.0230778\n",
      "Test:  0.29999998\n",
      "Epoch  480\n",
      "Training:  1.0154562\n",
      "Test:  0.29999998\n",
      "Epoch  490\n",
      "Training:  1.0079396\n",
      "Test:  0.29999998\n",
      "Epoch  500\n",
      "Training:  1.0005264\n",
      "Test:  0.29999998\n",
      "Epoch  510\n",
      "Training:  0.9932153\n",
      "Test:  0.29999998\n",
      "Epoch  520\n",
      "Training:  0.9860041\n",
      "Test:  0.29999998\n",
      "Epoch  530\n",
      "Training:  0.9788906\n",
      "Test:  0.29999998\n",
      "Epoch  540\n",
      "Training:  0.9718722\n",
      "Test:  0.29999998\n",
      "Epoch  550\n",
      "Training:  0.96494675\n",
      "Test:  0.29999998\n",
      "Epoch  560\n",
      "Training:  0.95811135\n",
      "Test:  0.29999998\n",
      "Epoch  570\n",
      "Training:  0.9513635\n",
      "Test:  0.29999998\n",
      "Epoch  580\n",
      "Training:  0.9447007\n",
      "Test:  0.29999998\n",
      "Epoch  590\n",
      "Training:  0.93812\n",
      "Test:  0.29999998\n",
      "Epoch  600\n",
      "Training:  0.93161887\n",
      "Test:  0.29999998\n",
      "Epoch  610\n",
      "Training:  0.92519456\n",
      "Test:  0.29999998\n",
      "Epoch  620\n",
      "Training:  0.9188449\n",
      "Test:  0.29999998\n",
      "Epoch  630\n",
      "Training:  0.9125669\n",
      "Test:  0.29999998\n",
      "Epoch  640\n",
      "Training:  0.90635824\n",
      "Test:  0.26666665\n",
      "Epoch  650\n",
      "Training:  0.90021706\n",
      "Test:  0.26666665\n",
      "Epoch  660\n",
      "Training:  0.89414054\n",
      "Test:  0.26666665\n",
      "Epoch  670\n",
      "Training:  0.8881265\n",
      "Test:  0.26666665\n",
      "Epoch  680\n",
      "Training:  0.8821732\n",
      "Test:  0.26666665\n",
      "Epoch  690\n",
      "Training:  0.87627846\n",
      "Test:  0.26666665\n",
      "Epoch  700\n",
      "Training:  0.87044036\n",
      "Test:  0.26666665\n",
      "Epoch  710\n",
      "Training:  0.8646572\n",
      "Test:  0.26666665\n",
      "Epoch  720\n",
      "Training:  0.8589274\n",
      "Test:  0.26666665\n",
      "Epoch  730\n",
      "Training:  0.8532492\n",
      "Test:  0.26666665\n",
      "Epoch  740\n",
      "Training:  0.8476212\n",
      "Test:  0.26666665\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-b3ef7cd07338>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\datasci\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1065\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[0;32m   1066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1067\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1068\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\datasci\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\datasci\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\datasci\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m_BiasAddGrad\u001b[1;34m(op, received_grad)\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[0mdata_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m   return (received_grad,\n\u001b[1;32m--> 351\u001b[1;33m           gen_nn_ops.bias_add_grad(\n\u001b[0m\u001b[0;32m    352\u001b[0m               out_backprop=received_grad, data_format=data_format))\n\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\datasci\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mbias_add_grad\u001b[1;34m(out_backprop, data_format, name)\u001b[0m\n\u001b[0;32m    750\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 752\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m    753\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"BiasAddGrad\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m         tld.op_callbacks, out_backprop, \"data_format\", data_format)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set input and output dim, the maximum number of iterations, the number of epochs and the optimizer\n",
    "threshold = 0.001\n",
    "learning_rate = 0.0001\n",
    "state_dim = 2\n",
    "input_dim = inp.shape[1]\n",
    "output_dim = labels.shape[1]\n",
    "max_it = 50\n",
    "num_epoch = 1000\n",
    "\n",
    "def create_model():\n",
    "\n",
    "    comp_inp = tf.keras.Input(shape=(input_dim), name=\"input\")\n",
    "    \n",
    "    layer = GraphNetwork(input_dim, state_dim, output_dim,                             \n",
    "                         hidden_state_dim = 5, hidden_output_dim = 25,\n",
    "                         ArcNode=arcnode,GraphNode=None,threshold=threshold)\n",
    "    \n",
    "    output = layer(comp_inp)\n",
    "    \n",
    "    model = tf.keras.Model(comp_inp, output)\n",
    "\n",
    "    return model,layer\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model,GNNLayer = create_model()\n",
    "\n",
    "# initialize GNN\n",
    "param = \"st_d\" + str(state_dim) + \"_th\" + str(threshold) + \"_lr\" + str(learning_rate)\n",
    "print(param)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "for count in range(num_epoch):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        out = model(inp,training=True)\n",
    "\n",
    "        loss_value = loss(labels,out, mask=mask_train)\n",
    "\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        if count % 10 == 0:\n",
    "            out_val = GNNLayer.predict_node(inp, arcnode)\n",
    "            loss_value_val = metric(labels,out, mask=mask_test)\n",
    "            \n",
    "            print(\"Epoch \", count)\n",
    "            print(\"Training: \", loss_value.numpy())\n",
    "            print(\"Test: \", loss_value_val.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 10,  7, 12])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(labels,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseMatrix(indices=array([[ 0,  0],\n",
       "       [ 0,  1],\n",
       "       [ 0,  2],\n",
       "       [ 0,  3],\n",
       "       [ 0,  4],\n",
       "       [ 0,  5],\n",
       "       [ 0,  6],\n",
       "       [ 0,  7],\n",
       "       [ 0,  8],\n",
       "       [ 0,  9],\n",
       "       [ 0, 10],\n",
       "       [ 0, 11],\n",
       "       [ 0, 12],\n",
       "       [ 0, 13],\n",
       "       [ 0, 14],\n",
       "       [ 0, 15],\n",
       "       [ 1, 16],\n",
       "       [ 1, 17],\n",
       "       [ 1, 18],\n",
       "       [ 1, 19],\n",
       "       [ 1, 20],\n",
       "       [ 1, 21],\n",
       "       [ 1, 22],\n",
       "       [ 1, 23],\n",
       "       [ 2, 24],\n",
       "       [ 2, 25],\n",
       "       [ 2, 26],\n",
       "       [ 2, 27],\n",
       "       [ 2, 28],\n",
       "       [ 2, 29],\n",
       "       [ 2, 30],\n",
       "       [ 2, 31],\n",
       "       [ 3, 32],\n",
       "       [ 3, 33],\n",
       "       [ 3, 34],\n",
       "       [ 4, 35],\n",
       "       [ 4, 36],\n",
       "       [ 5, 37],\n",
       "       [ 5, 38],\n",
       "       [ 5, 39],\n",
       "       [ 6, 40],\n",
       "       [ 8, 41],\n",
       "       [ 8, 42],\n",
       "       [ 8, 43],\n",
       "       [ 9, 44],\n",
       "       [13, 45],\n",
       "       [14, 46],\n",
       "       [14, 47],\n",
       "       [15, 48],\n",
       "       [15, 49],\n",
       "       [18, 50],\n",
       "       [18, 51],\n",
       "       [19, 52],\n",
       "       [20, 53],\n",
       "       [20, 54],\n",
       "       [22, 55],\n",
       "       [22, 56],\n",
       "       [23, 57],\n",
       "       [23, 58],\n",
       "       [23, 59],\n",
       "       [23, 60],\n",
       "       [23, 61],\n",
       "       [24, 62],\n",
       "       [24, 63],\n",
       "       [24, 64],\n",
       "       [25, 65],\n",
       "       [26, 66],\n",
       "       [26, 67],\n",
       "       [27, 68],\n",
       "       [28, 69],\n",
       "       [28, 70],\n",
       "       [29, 71],\n",
       "       [29, 72],\n",
       "       [30, 73],\n",
       "       [30, 74],\n",
       "       [31, 75],\n",
       "       [31, 76],\n",
       "       [32, 77]]), values=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32), dense_shape=[34, 78])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arcnode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "datasci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
