{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gnn.gnn_utils as gnn_utils\n",
    "from gnn.GNN import GNN as GraphNetwork\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "##### GPU & stuff config\n",
    "import os\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "data_path = \"./data\"\n",
    "\n",
    "set_name = \"sub_15_7_200\"\n",
    "############# training set ################\n",
    "\n",
    "\n",
    "#inp, arcnode, nodegraph, nodein, labels = Library.set_load_subgraph(data_path, \"train\")\n",
    "inp, arcnode, nodegraph, nodein, labels, _ = gnn_utils.set_load_general(data_path, \"train\", set_name=set_name)\n",
    "############ test set ####################\n",
    "\n",
    "#inp_test, arcnode_test, nodegraph_test, nodein_test, labels_test = Library.set_load_subgraph(data_path, \"test\")\n",
    "inp_test, arcnode_test, nodegraph_test, nodein_test, labels_test, _ = gnn_utils.set_load_general(data_path, \"test\", set_name=set_name)\n",
    "\n",
    "############ validation set #############\n",
    "\n",
    "#inp_val, arcnode_val, nodegraph_val, nodein_val, labels_val = Library.set_load_subgraph(data_path, \"valid\")\n",
    "inp_val, arcnode_val, nodegraph_val, nodein_val, labels_val, _ = gnn_utils.set_load_general(data_path, \"validation\", set_name=set_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 0.00000001\n",
    "\n",
    "@tf.function()\n",
    "def loss(target,output):\n",
    "    target = tf.cast(target,tf.float32)\n",
    "    output = tf.maximum(output, EPSILON, name=\"Avoiding_explosions\")  # to avoid explosions\n",
    "    xent = -tf.reduce_sum(target * tf.math.log(output), 1)\n",
    "    lo = tf.reduce_mean(xent)\n",
    "    return lo\n",
    "\n",
    "@tf.function()\n",
    "def metric(output, target):\n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(target, 1))\n",
    "    metric = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "    return metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = inp[0]\n",
    "\n",
    "arcnode=arcnode[0]\n",
    "\n",
    "nodegraph=nodegraph[0]\n",
    "\n",
    "inp_val = inp_val[0]\n",
    "\n",
    "arcnode_val = arcnode_val[0]\n",
    "\n",
    "nodegraph_val=nodegraph_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set input and output dim, the maximum number of iterations, the number of epochs and the optimizer\n",
    "\n",
    "threshold = 0.001\n",
    "learning_rate = 0.001\n",
    "state_dim = 10\n",
    "\n",
    "input_dim = len(inp[0])\n",
    "output_dim = 2\n",
    "max_it = 50\n",
    "num_epoch = 5000\n",
    "\n",
    "def create_model():\n",
    "\n",
    "    comp_inp = tf.keras.Input(shape=(input_dim), name=\"input\")\n",
    "    \n",
    "    layer = GraphNetwork(input_dim, state_dim, output_dim,                             \n",
    "                         hidden_state_dim = 5, hidden_output_dim = 5,\n",
    "                         ArcNode=arcnode,NodeGraph=None,threshold=threshold)\n",
    "    \n",
    "    output = layer(comp_inp)\n",
    "    \n",
    "    model = tf.keras.Model(comp_inp, output)\n",
    "\n",
    "    return model,layer\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model,GNNLayer = create_model()\n",
    "\n",
    "# initialize GNN\n",
    "param = \"st_d\" + str(state_dim) + \"_th\" + str(threshold) + \"_lr\" + str(learning_rate)\n",
    "print(param)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "for count in range(0, num_epoch):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        out = model(inp.astype(np.float32),training=True)\n",
    "\n",
    "        loss_value = loss(labels,out)\n",
    "\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if count % 30 == 0:\n",
    "            out_val = GNNLayer.predict_node(inp_val.astype(np.float32), arcnode_val)\n",
    "            loss_value_val = loss(labels_val,out_val)\n",
    "            \n",
    "            print(\"Epoch \", count)\n",
    "            print(\"Training: \", loss_value.numpy())\n",
    "            print(\"Validation: \",loss_value_val.numpy())\n",
    "\n",
    "        count = count + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcnode_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35294117647058826"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12/np.sum(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "datasci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
